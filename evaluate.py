import os
import sys
import logging
import argparse
import random
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Subset
from torchvision import transforms
from tensorboardX import SummaryWriter
from utils import (
    generalized_energy_distance_iou, hm_iou_cal, dice_max_cal2
)
from load_LIDC_data import LIDC_IDRI, RandomGenerator
from p2sam import P2SAM

# Configure logging
logging.basicConfig(filename='evaluation_log.txt', level=logging.INFO, format='%(asctime)s - %(message)s')

# Parse command-line arguments
parser = argparse.ArgumentParser(description='Evaluate the model with specified epochs and weights.')
parser.add_argument('--epochs', nargs='+', type=int, default=[10, 50, 70, 100], help='Epochs to load weights from.')
parser.add_argument('--combined_weights_path', type=str, default='best/best_prior_checkpoint.pth', help='Path to the combined weights file.')
parser.add_argument('--weight_eight_path', type=str, default='best/best_weights.pt', help='Path to the weight_eight file.')
parser.add_argument('--lora_sam_weights_path', type=str, default='best/best_lora_checkpoint.pth', help='Path to the lora_sam weights file.')
parser.add_argument('--gpuid', type=int, default=3, help='ID of the GPU to use.')
parser.add_argument('--batch_size', type=int, default=8, help='Batch size for data loading.')
parser.add_argument('--total_samples', type=int, default=16, help='Total number of samples to generate.')
args = parser.parse_args()

# Set device
device = torch.device(f'cuda:{args.gpuid}' if torch.cuda.is_available() else 'cpu')

combined_weights = torch.load(args.combined_weights_path, map_location=device)
weight_eight = torch.load(args.weight_eight_path, map_location=device)

# Initialize networks
def initialize_networks(epochs):
    networks = []
    for epoch in epochs:
        epoch_key = f'epoch_{epoch}'
        if epoch_key in combined_weights:
            net = P2SAM(device=device, lora_ckpt=args.lora_sam_weights_path)  # Initialize network
            non_lora_weights = combined_weights[epoch_key]
            net.load_state_dict(non_lora_weights, strict=False)  # Load non-lora_sam weights
            net.to(device)
            networks.append(net)
        else:
            print(f"Warning: Weights for epoch {epoch} not found.")
    return networks

ged_score = dice_max2_score = hm_iou_score = 0
networks = initialize_networks(args.epochs)

# Log the weights being used
logging.info(f"Using combined weights from {args.combined_weights_path} for epochs: {args.epochs}")

# Prepare dataset
low_res = networks[0].img_embedding_size * 4
db = LIDC_IDRI(dataset_location='/data/cxli/yuzhi/samed/SAMed-main/data/', transform=transforms.Compose([
    RandomGenerator(output_size=[128, 128], low_res=[low_res, low_res], test=True)
]))
dataset_size = len(db)
indices = list(range(dataset_size))
train_split = int(np.floor(0.6 * dataset_size))
validation_split = int(np.floor(0.8 * dataset_size))
train_indices = indices[:train_split]
validation_indices = indices[train_split:validation_split]
test_indices = indices[validation_split:]

train_dataset = Subset(db, train_indices)
validation_dataset = Subset(db, validation_indices)
test_dataset = Subset(db, test_indices)

train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)
validation_loader = DataLoader(validation_dataset, batch_size=args.batch_size, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)

print(f"Total dataset size: {dataset_size}")
print(f"Training set size: {len(train_indices)}")
print(f"Validation set size: {len(validation_indices)}")
print(f"Test set size: {len(test_indices)}")

# Hyperparameter: total number of samples
samples_per_net = args.total_samples // len(networks)  # Number of samples generated by each network

# Process each batch and evaluate metrics
for i_batch, sampled_batch in enumerate(test_loader):
    print(f'Processing batch {i_batch}')
    logging.info(f'Processing batch {i_batch}')
    image_batch, label_batch = sampled_batch['image'].to(device), sampled_batch['label'].to(device)
    label_four_batch = sampled_batch['label_four']
    low_res_label_batch = sampled_batch['low_res_label'].to(device)
    assert image_batch.max() <= 3, f'image_batch max: {image_batch.max()}'
    
    pred_list = [[] for _ in range(image_batch.shape[0])]
    
    for i in range(samples_per_net):
        image_batch_oc = sampled_batch['image_oc'].to(device)
        outputs = [net.forward(image_batch, image_batch_oc, train=False) for net in networks]
        
        for j, output in enumerate(outputs):
            logits_high = output['masks'].to(device) * weight_eight.unsqueeze(-1)  # Apply weight_eight
            logits_high_res = logits_high.sum(1).unsqueeze(1)
            
            for k in range(image_batch.shape[0]):
                pred_list[k].append(logits_high_res[k])
    
    for index in range(len(pred_list)):
        label_four_filter = label_four_batch[index]
        pred_eval = torch.cat(pred_list[index], 0)
        pred_eval = (pred_eval > 0).cpu().detach().int()
        
        iou_score_iter, ged_score_iter = generalized_energy_distance_iou(pred_eval, label_four_filter)
        score, _ = hm_iou_cal(pred_eval, label_four_filter)
        hm_iou_score += score
        dice_max2_score += dice_max_cal2(pred_eval, label_four_filter)
        ged_score += ged_score_iter

# Calculate average scores
ged = ged_score / len(test_indices)
dice_max2 = dice_max2_score / len(test_indices)
hm_iou = hm_iou_score / len(test_indices)

print(f"ged_score: {ged}, dice_max_score2: {dice_max2}, hm_iou_score: {hm_iou}")
logging.info(f"ged_score: {ged}, dice_max_score2: {dice_max2}, hm_iou_score: {hm_iou}")